{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Seperate train and test",
   "id": "e6a538426e47fd81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T05:19:05.678206Z",
     "start_time": "2025-07-29T05:18:52.382323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from src.graph_to_vec_converter import  HVs\n",
    "# from sklearn.metrics       import accuracy_score, classification_report\n",
    "# from sklearn.linear_model  import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from src.graph_generation import NMNISTGraphDataset\n",
    "# from src.loader import ev_loader\n",
    "# from src.graphcnnVSA_Binding_FULL import GraphCNN\n",
    "# from src.codebook import CodeBook\n",
    "# import torch\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing   import StandardScaler\n",
    "# from sklearn.svm             import SVC\n",
    "# from sklearn.pipeline        import Pipeline\n",
    "# from sklearn.metrics         import accuracy_score\n",
    "#\n",
    "#\n",
    "# print(\"[LOG] - parameter initialization.\")\n",
    "# # GRAPH parameters\n",
    "# DATA_NAME = \"SNKTH\" # NCARS, NMNIST, SNKTH\n",
    "# DATA_PATH = \"data\"\n",
    "# DATASET = \"full\"  # full / test      size of dataset loading for training and testing\n",
    "# NUM_OF_GRAPH_EVENTS = 100\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "# NORMALIZE_FEAT = False\n",
    "#\n",
    "# if DATA_NAME == \"NCARS\":\n",
    "#     X_MAX = 360\n",
    "#     Y_MAX = 360\n",
    "#     T_MAX = 100_000\n",
    "#     T_STEP = 10_0\n",
    "#\n",
    "#     R = 7\n",
    "#     D_MAX = 30\n",
    "#\n",
    "#     # NOISE parameters\n",
    "#     NOICE_REMOVED = True\n",
    "#     NR_BIN_XY_SIZE = 4\n",
    "#     NR_TIME_BIN_SIZE = 20_000\n",
    "#     NR_MINIMUM_EVENTS = 3\n",
    "#\n",
    "# if DATA_NAME == \"NCARS\":\n",
    "#     X_MAX = 200\n",
    "#     Y_MAX = 200\n",
    "#     T_MAX = 100_000_000\n",
    "#     T_STEP = 10_000\n",
    "#\n",
    "#     R = 7\n",
    "#     D_MAX = 30\n",
    "#\n",
    "#     # NOISE parameters\n",
    "#     NOICE_REMOVED = True\n",
    "#     NR_BIN_XY_SIZE = 4\n",
    "#     NR_TIME_BIN_SIZE = 20_000\n",
    "#     NR_MINIMUM_EVENTS = 3\n",
    "#\n",
    "#\n",
    "# # GVFA parameters\n",
    "# HV_DIMENTION = 1000\n",
    "# LAYERS = 5\n",
    "# DELTA = 1  # 2\n",
    "# EQUATION = 11\n",
    "#\n",
    "#\n",
    "# # load event streams\n",
    "# print(\"[LOG] - Loading events\")\n",
    "# # full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\n",
    "# train_ds, test_ds = ev_loader(root=DATA_PATH, dataset=DATASET, data_name= DATA_NAME )\n",
    "#\n",
    "# print(\"[LOG] - Making class objects.\")\n",
    "# MNISTGraph_model_train = NMNISTGraphDataset(tonic_raw_dataset=train_ds, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "#                                       R=R, Dmax=D_MAX,\n",
    "#                                       noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "#                                       nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "#                                       nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "#\n",
    "# MNISTGraph_model_test = NMNISTGraphDataset(tonic_raw_dataset=test_ds, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "#                                             R=R, Dmax=D_MAX,\n",
    "#                                             noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "#                                             nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "#                                             nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "#\n",
    "# MNISTGraph_model_test_50 = NMNISTGraphDataset(tonic_raw_dataset=test_ds, num_of_graph_events=50,\n",
    "#                                             R=R, Dmax=D_MAX,\n",
    "#                                             noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "#                                             nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "#                                             nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "#\n",
    "# MNISTGraph_model_test_10 = NMNISTGraphDataset(tonic_raw_dataset=test_ds, num_of_graph_events=10,\n",
    "#                                             R=R, Dmax=D_MAX,\n",
    "#                                             noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "#                                             nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "#                                             nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "#\n",
    "# gvfa_model = GraphCNN(input_dim=HV_DIMENTION, num_layers=LAYERS, delta=DELTA, graph_pooling_type=\"sum\",\n",
    "#                       neighbor_pooling_type=\"sum\", device=DEVICE, equation=EQUATION).to(DEVICE)\n",
    "# cb = CodeBook(dim=HV_DIMENTION, x_max=X_MAX, y_max=Y_MAX, t_max=T_MAX, t_step=T_STEP)\n",
    "# hvs = HVs(codebook=cb, gvfa_model=gvfa_model)\n",
    "#\n",
    "#\n",
    "# X_train_, X_test_,X_test_50_,X_test_10_, y_train, y_test = [],[],[],[],[],[]\n",
    "# print(\"[LOG] - Loading graph and converting to HVs.\")\n",
    "# for i in range(len(train_ds)):\n",
    "#     print(i)\n",
    "#     g = MNISTGraph_model_train.get(i)\n",
    "#     x, y = hvs.make_hvs(graph=g)\n",
    "#     X_train_.append(x)\n",
    "#     y_train.append(y)\n",
    "#\n",
    "#\n",
    "# for i in range(len(test_ds)):\n",
    "#     print(i)\n",
    "#     g = MNISTGraph_model_test.get(i)\n",
    "#     g_50 = MNISTGraph_model_test_50.get(i)\n",
    "#     g_10 = MNISTGraph_model_test_10.get(i)\n",
    "#\n",
    "#     # print(g)\n",
    "#     x, y = hvs.make_hvs(graph=g)\n",
    "#     x_50, _ = hvs.make_hvs(graph=g_50)\n",
    "#     x_10, _ = hvs.make_hvs(graph=g_10)\n",
    "#\n",
    "#     X_test_.append(x)\n",
    "#     X_test_50_.append(x_50)\n",
    "#     X_test_10_.append(x_10)\n",
    "#     y_test.append(y)\n",
    "#\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train_)\n",
    "#\n",
    "# # scaler1 = StandardScaler()\n",
    "# X_test = scaler.fit_transform(X_test_)\n",
    "# X_test_50 = scaler.fit_transform(X_test_50_)\n",
    "# X_test_10 = scaler.fit_transform(X_test_10_)\n",
    "#\n",
    "# del cb\n",
    "# del hvs\n",
    "# del gvfa_model\n",
    "# # del full_ev_ds\n",
    "#\n",
    "# el = [1000]\n",
    "# for elm in el:\n",
    "#     print(elm)\n",
    "#     print(\"[LOG] - Classification.\")\n",
    "#\n",
    "#     # clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.9,degree=6)\n",
    "#     pipe_rbf = Pipeline([\n",
    "#         (\"sc\", StandardScaler(with_mean=False)),\n",
    "#         (\"svc\", SVC(kernel=\"rbf\"))\n",
    "#     ])\n",
    "#\n",
    "#     param_grid = {\n",
    "#         \"svc__C\": [0.1, 1, 3],\n",
    "#         \"svc__gamma\": [\"scale\", 1e-3, 1e-2, 1e-1],\n",
    "#         \"svc__class_weight\": [None, \"balanced\"]\n",
    "#     }\n",
    "#\n",
    "#     grid = GridSearchCV(pipe_rbf,\n",
    "#                         param_grid=param_grid,\n",
    "#                         n_jobs=-1,\n",
    "#                         verbose=1)\n",
    "#     grid.fit(X_train, y_train)\n",
    "#\n",
    "#     print(grid.best_params_)\n",
    "#     print(grid.best_score_)\n",
    "#     print(grid.param_grid)\n",
    "#\n",
    "#     print(\"----100------\")\n",
    "#     print(f\"Train accuracy: {accuracy_score(y_train, grid.predict(X_train)) * 100:.2f}%\")\n",
    "#     print(f\"Test  accuracy: {accuracy_score(y_test, grid.predict(X_test)) * 100:.2f}%\")\n",
    "#\n",
    "#     print(\"----50------\")\n",
    "#     print(f\"Test  accuracy: {accuracy_score(y_test, grid.predict(X_test_50)) * 100:.2f}%\")\n",
    "#\n",
    "#     print(\"----10------\")\n",
    "#     print(f\"Test  accuracy: {accuracy_score(y_test, grid.predict(X_test_10)) * 100:.2f}%\")\n",
    "#\n",
    "#     print(\"[LOG]- NUM_OF_GRAPH_EVENTS:\", NUM_OF_GRAPH_EVENTS, \" | DATASET:\", DATASET,\n",
    "#           \" | NORMALIZE_FEAT:\", NORMALIZE_FEAT,\n",
    "#           \" | R:\", R, \" | D_MAX: \", D_MAX, \" | NOICE_REMOVED: \", NOICE_REMOVED,\n",
    "#           \" | NR_BIN_XY_SIZE: \", NR_BIN_XY_SIZE, \" | NR_TIME_BIN_SIZE: \", NR_TIME_BIN_SIZE, \" | NR_MINIMUM_EVENTS: \",\n",
    "#           NR_MINIMUM_EVENTS, \" | HV_DIMENTION: \", HV_DIMENTION,\" | LAYERS: \", LAYERS,\" | DELTA: \", DELTA,\" | EQUATION: \", EQUATION,)\n",
    "#\n",
    "#     # del clf\n"
   ],
   "id": "26b7c82b3fa99e60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - parameter initialization.\n",
      "[LOG] - Loading events\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 68\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LOG] - Loading events\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\u001B[39;00m\n\u001B[1;32m---> 68\u001B[0m train_ds, test_ds \u001B[38;5;241m=\u001B[39m ev_loader(root\u001B[38;5;241m=\u001B[39mDATA_PATH, dataset\u001B[38;5;241m=\u001B[39mDATASET, data_name\u001B[38;5;241m=\u001B[39m DATA_NAME )\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LOG] - Making class objects.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     71\u001B[0m MNISTGraph_model_train \u001B[38;5;241m=\u001B[39m NMNISTGraphDataset(tonic_raw_dataset\u001B[38;5;241m=\u001B[39mtrain_ds, num_of_graph_events\u001B[38;5;241m=\u001B[39mNUM_OF_GRAPH_EVENTS,\n\u001B[0;32m     72\u001B[0m                                       R\u001B[38;5;241m=\u001B[39mR, Dmax\u001B[38;5;241m=\u001B[39mD_MAX,\n\u001B[0;32m     73\u001B[0m                                       noise_remove\u001B[38;5;241m=\u001B[39mNOICE_REMOVED, normalized_feat\u001B[38;5;241m=\u001B[39mNORMALIZE_FEAT,\n\u001B[0;32m     74\u001B[0m                                       nr_bin_xy_size\u001B[38;5;241m=\u001B[39mNR_BIN_XY_SIZE, nr_minimum_events\u001B[38;5;241m=\u001B[39mNR_MINIMUM_EVENTS,\n\u001B[0;32m     75\u001B[0m                                       nr_time_bin_size\u001B[38;5;241m=\u001B[39mNR_TIME_BIN_SIZE)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T10:19:12.652833Z",
     "start_time": "2025-07-25T08:20:05.720557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "#\n",
    "#\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=1000) # Increase max_iter for\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'penalty': ['l1'],\n",
    "#         'solver': ['liblinear', 'saga'], # Solvers that support L1 penalty\n",
    "#         'C': np.logspace(-4, 4, 10),\n",
    "#     },\n",
    "#     {\n",
    "#         'penalty': ['l2'],\n",
    "#         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # Solvers that support L2 penalty\n",
    "#         'C': np.logspace(-4, 4, 10),\n",
    "#     },\n",
    "#     {\n",
    "#         'penalty': ['elasticnet'],\n",
    "#         'solver': ['saga'], # Only 'saga' supports elasticnet\n",
    "#         'C': np.logspace(-4, 4, 10),\n",
    "#         'l1_ratio': np.linspace(0.1, 0.9, 5) # Mixing ratio for L1/L2 in elasticnet\n",
    "#     }\n",
    "# ]\n",
    "#\n",
    "#\n",
    "# cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5-fold cross-validation\n",
    "#\n",
    "# grid_search = GridSearchCV(estimator=log_reg,\n",
    "#                            param_grid=param_grid,\n",
    "#                            cv=cv_strategy,\n",
    "#                            scoring='accuracy', # Or 'f1_macro', 'roc_auc', etc.\n",
    "#                            verbose=1,\n",
    "#                            n_jobs=-1) # Use all available CPU cores\n",
    "#\n",
    "# # --- 6. Fit GridSearchCV to the scaled training data ---\n",
    "# print(\"\\nStarting GridSearchCV...\")\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# print(\"GridSearchCV finished.\")\n",
    "#\n",
    "# # --- 7. Get the Best Parameters and Best Score ---\n",
    "# print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "# print(\"Best cross-validation accuracy: {:.4f}\".format(grid_search.best_score_))\n",
    "#\n",
    "# # --- 8. Evaluate the Best Model on the Test Set ---\n",
    "# # The best_estimator_ attribute holds the model trained with the best parameters on the full training data.\n",
    "# best_log_reg_model = grid_search.best_estimator_\n",
    "# y_pred = best_log_reg_model.predict(X_test)\n",
    "#\n",
    "# print(\"\\n--- Test Set Evaluation ---\")\n",
    "# test_accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# print(\"---------------50---------------\")\n",
    "# test_accuracy = accuracy_score(y_test, best_log_reg_model.predict(X_test_50))\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "#\n",
    "# print(\"---------------50---------------\")\n",
    "# test_accuracy = accuracy_score(y_test, best_log_reg_model.predict(X_test_10))\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "cf2a3f412ba68999",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting GridSearchCV...\n",
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "GridSearchCV finished.\n",
      "\n",
      "Best parameters found:  {'C': 0.005994842503189409, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best cross-validation accuracy: 0.7878\n",
      "\n",
      "--- Test Set Evaluation ---\n",
      "Test Accuracy: 0.7279\n",
      "---------------50---------------\n",
      "Test Accuracy: 0.7144\n",
      "---------------50---------------\n",
      "Test Accuracy: 0.6589\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Shuffle train and test dara",
   "id": "8d1ce511c9637b5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T02:06:26.055281Z",
     "start_time": "2025-07-30T01:46:45.479936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.graph_to_vec_converter import  HVs\n",
    "from sklearn.metrics       import accuracy_score, classification_report\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from src.graph_generation import NMNISTGraphDataset\n",
    "from src.loader import ev_loader\n",
    "from src.graphcnnVSA_Binding_FULL import GraphCNN\n",
    "from src.codebook import CodeBook\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from torch.utils.data import ConcatDataset\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "\n",
    "print(\"[LOG] - parameter initialization.\")\n",
    "# GRAPH parameters\n",
    "DATA_NAME = \"SNKTH\" # NCARS, NMNIST\n",
    "DATA_PATH = \"data\"\n",
    "DATASET = \"full\"  # full / test      size of dataset loading for training and testing\n",
    "NORMALIZE_FEAT = False\n",
    "NUM_OF_GRAPH_EVENTS = 100  # None, 10, 50, 100. etc\n",
    "\n",
    "if DATA_NAME == \"SNKTH\":\n",
    "    X_MAX = 360\n",
    "    Y_MAX = 360\n",
    "    T_MAX = 1_000_000\n",
    "    T_STEP = 10_00\n",
    "\n",
    "    R = 7\n",
    "    D_MAX = 30\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = True\n",
    "    NR_BIN_XY_SIZE = 4\n",
    "    NR_TIME_BIN_SIZE = 20_000\n",
    "    NR_MINIMUM_EVENTS = 3\n",
    "\n",
    "if DATA_NAME == \"NCARS\":\n",
    "    X_MAX = 200\n",
    "    Y_MAX = 200\n",
    "    T_MAX = 100_000_000\n",
    "    T_STEP = 10_000\n",
    "\n",
    "    R = 13\n",
    "    D_MAX = 20\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = True\n",
    "    NR_BIN_XY_SIZE = 4\n",
    "    NR_TIME_BIN_SIZE = 20_000\n",
    "    NR_MINIMUM_EVENTS = 3\n",
    "\n",
    "\n",
    "# GVFA parameters\n",
    "HV_DIMENTION = 1000\n",
    "LAYERS = 5\n",
    "DELTA = 1  # 2\n",
    "EQUATION = 11\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# load event streams\n",
    "print(\"[LOG] - Loading events\")\n",
    "# full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\n",
    "ds = ev_loader(root=DATA_PATH, dataset=DATASET, data_name=DATA_NAME)\n",
    "ds_train, ds_test = train_test_split(ds, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"[LOG] - Making class objects.\")\n",
    "MNISTGraph_model_train_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_train, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MNISTGraph_model_test_50 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=50,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_10 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=10,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "items = [5000]\n",
    "for item in items:\n",
    "    HV_DIMENTION = item\n",
    "    gvfa_model = GraphCNN(input_dim=HV_DIMENTION, num_layers=LAYERS, delta=DELTA, graph_pooling_type=\"sum\",\n",
    "                          neighbor_pooling_type=\"sum\", device=DEVICE, equation=EQUATION).to(DEVICE)\n",
    "    cb = CodeBook(dim=HV_DIMENTION, x_max=X_MAX, y_max=Y_MAX, t_max=T_MAX, t_step=T_STEP)\n",
    "    hvs = HVs(codebook=cb, gvfa_model=gvfa_model)\n",
    "\n",
    "\n",
    "    X_train_100, X_test_100, X_test_50,X_test_10, Y_train_100,Y_test_100, y_test_50_10 = [],[],[],[], [],[],[]\n",
    "    print(\"[LOG] - Loading graph and converting to HVs.\")\n",
    "    for i in range(len(ds_train)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_train_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_train_100.append(x)\n",
    "        Y_train_100.append(y)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_test_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_test_100.append(x)\n",
    "        Y_test_100.append(y)\n",
    "\n",
    "    # X_train_100, X_test_100, Y_train_100, Y_test_100 = train_test_split(X_train_100, Y_train_100, test_size=0.2, random_state=42, shuffle=True)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g_50 = MNISTGraph_model_test_50.get(i)\n",
    "        g_10 = MNISTGraph_model_test_10.get(i)\n",
    "\n",
    "        # print(g)\n",
    "\n",
    "        x_50, y = hvs.make_hvs(graph=g_50)\n",
    "        x_10, _ = hvs.make_hvs(graph=g_10)\n",
    "\n",
    "\n",
    "        X_test_50.append(x_50)\n",
    "        X_test_10.append(x_10)\n",
    "        y_test_50_10.append(y)\n",
    "\n",
    "\n",
    "    # X_test_50 = scaler.fit_transform(X_test_50_)\n",
    "    # X_test_10 = scaler.fit_transform(X_test_10_)\n",
    "\n",
    "\n",
    "    del cb\n",
    "    del hvs\n",
    "    del gvfa_model\n",
    "    # del full_ev_ds\n",
    "\n",
    "    el = [5000]\n",
    "\n",
    "    print(\"[LOG] - Classification.\")\n",
    "\n",
    "    # clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.9,degree=6)\n",
    "    pipe_rbf = Pipeline([\n",
    "        (\"vt\", VarianceThreshold(threshold=1e-6)),\n",
    "        (\"skb\", SelectKBest(mutual_info_classif, k=500)),\n",
    "        (\"sc\", StandardScaler(with_mean=False)),\n",
    "        (\"svc\", SVC(kernel=\"rbf\"))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"svc__C\": [ 0.5, 0.8, 1, 10],\n",
    "        \"svc__gamma\": [\"scale\",\"auto\", 1e-4, 1e-3, 1e-2],\n",
    "        \"svc__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(pipe_rbf,\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=1)\n",
    "    grid.fit(X_train_100, Y_train_100)\n",
    "\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    print(grid.param_grid)\n",
    "\n",
    "    print(\"----100------\")\n",
    "    print(f\"Train accuracy: {accuracy_score(Y_train_100, grid.predict(X_train_100)) * 100:.2f}%\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(Y_test_100, grid.predict(X_test_100)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----50------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_50)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----10------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_10)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"[LOG]- NUM_OF_GRAPH_EVENTS:\", NUM_OF_GRAPH_EVENTS, \" | DATASET:\", DATASET,\n",
    "          \" | NORMALIZE_FEAT:\", NORMALIZE_FEAT,\n",
    "          \" | R:\", R, \" | D_MAX: \", D_MAX, \" | NOICE_REMOVED: \", NOICE_REMOVED,\n",
    "          \" | NR_BIN_XY_SIZE: \", NR_BIN_XY_SIZE, \" | NR_TIME_BIN_SIZE: \", NR_TIME_BIN_SIZE, \" | NR_MINIMUM_EVENTS: \",\n",
    "          NR_MINIMUM_EVENTS, \" | HV_DIMENTION: \", HV_DIMENTION,\" | LAYERS: \", LAYERS,\" | DELTA: \", DELTA,\" | EQUATION: \", EQUATION,)\n",
    "\n",
    "        # del clf\n"
   ],
   "id": "ecc51e074c73d68b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - parameter initialization.\n",
      "[LOG] - Loading events\n",
      "[LOG] - Making class objects.\n",
      "[LOG] - Loading graph and converting to HVs.\n",
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "{'svc__C': 10, 'svc__class_weight': 'balanced', 'svc__gamma': 'auto'}\n",
      "0.5806732659841345\n",
      "{'svc__C': [0.5, 0.8, 1, 10], 'svc__gamma': ['scale', 'auto', 0.0001, 0.001, 0.01], 'svc__class_weight': [None, 'balanced']}\n",
      "----100------\n",
      "Train accuracy: 94.87%\n",
      "Test  accuracy: 61.16%\n",
      "----50------\n",
      "Test  accuracy: 58.84%\n",
      "----10------\n",
      "Test  accuracy: 56.05%\n",
      "[LOG]- NUM_OF_GRAPH_EVENTS: 100  | DATASET: full  | NORMALIZE_FEAT: False  | R: 7  | D_MAX:  30  | NOICE_REMOVED:  True  | NR_BIN_XY_SIZE:  4  | NR_TIME_BIN_SIZE:  20000  | NR_MINIMUM_EVENTS:  3  | HV_DIMENTION:  5000  | LAYERS:  5  | DELTA:  1  | EQUATION:  11\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# svc single classifier",
   "id": "2cfa9387c42690e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:22:16.640959Z",
     "start_time": "2025-07-31T02:48:19.203860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.graph_to_vec_converter import  HVs\n",
    "from sklearn.metrics       import accuracy_score, classification_report\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from src.graph_generation import NMNISTGraphDataset\n",
    "from src.loader import ev_loader\n",
    "from src.graphcnnVSA_Binding_FULL import GraphCNN\n",
    "from src.codebook import CodeBook\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from torch.utils.data import ConcatDataset\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "\n",
    "print(\"[LOG] - parameter initialization.\")\n",
    "# GRAPH parameters\n",
    "DATA_NAME = \"SNKTH\" # NCARS, NMNIST\n",
    "DATA_PATH = \"data\"\n",
    "DATASET = \"full\"  # full / test      size of dataset loading for training and testing\n",
    "NORMALIZE_FEAT = False\n",
    "NUM_OF_GRAPH_EVENTS = None  # None, 10, 50, 100. etc\n",
    "\n",
    "if DATA_NAME == \"SNKTH\":\n",
    "    X_MAX = 360\n",
    "    Y_MAX = 360\n",
    "    T_MAX = 1_000_000\n",
    "    T_STEP = 10_0\n",
    "\n",
    "    R = 2\n",
    "    D_MAX = 4\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = False\n",
    "    NR_BIN_XY_SIZE = 5\n",
    "    NR_TIME_BIN_SIZE = 20_00\n",
    "    NR_MINIMUM_EVENTS = 2\n",
    "\n",
    "\n",
    "# GVFA parameters\n",
    "HV_DIMENTION = 1000\n",
    "LAYERS = 5\n",
    "DELTA = 1  # 2\n",
    "EQUATION = 11\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# load event streams\n",
    "print(\"[LOG] - Loading events\")\n",
    "# full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\n",
    "ds = ev_loader(root=DATA_PATH, dataset=DATASET, data_name=DATA_NAME)\n",
    "ds_train, ds_test = train_test_split(ds, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"[LOG] - Making class objects.\")\n",
    "MNISTGraph_model_train_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_train, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MNISTGraph_model_test_50 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=50,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_10 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=10,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "items = [5000]\n",
    "for item in items:\n",
    "    HV_DIMENTION = item\n",
    "    gvfa_model = GraphCNN(input_dim=HV_DIMENTION, num_layers=LAYERS, delta=DELTA, graph_pooling_type=\"sum\",\n",
    "                          neighbor_pooling_type=\"sum\", device=DEVICE, equation=EQUATION).to(DEVICE)\n",
    "    cb = CodeBook(dim=HV_DIMENTION, x_max=X_MAX, y_max=Y_MAX, t_max=T_MAX, t_step=T_STEP)\n",
    "    hvs = HVs(codebook=cb, gvfa_model=gvfa_model)\n",
    "\n",
    "\n",
    "    X_train_100, X_test_100, X_test_50,X_test_10, Y_train_100,Y_test_100, y_test_50_10 = [],[],[],[], [],[],[]\n",
    "    print(\"[LOG] - Loading graph and converting to HVs.\")\n",
    "    for i in range(len(ds_train)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_train_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_train_100.append(x)\n",
    "        Y_train_100.append(y)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_test_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_test_100.append(x)\n",
    "        Y_test_100.append(y)\n",
    "\n",
    "    # X_train_100, X_test_100, Y_train_100, Y_test_100 = train_test_split(X_train_100, Y_train_100, test_size=0.2, random_state=42, shuffle=True)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g_50 = MNISTGraph_model_test_50.get(i)\n",
    "        g_10 = MNISTGraph_model_test_10.get(i)\n",
    "\n",
    "        # print(g)\n",
    "\n",
    "        x_50, y = hvs.make_hvs(graph=g_50)\n",
    "        x_10, _ = hvs.make_hvs(graph=g_10)\n",
    "\n",
    "\n",
    "        X_test_50.append(x_50)\n",
    "        X_test_10.append(x_10)\n",
    "        y_test_50_10.append(y)\n",
    "\n",
    "\n",
    "    # X_test_50 = scaler.fit_transform(X_test_50_)\n",
    "    # X_test_10 = scaler.fit_transform(X_test_10_)\n",
    "\n",
    "\n",
    "    del cb\n",
    "    del hvs\n",
    "    del gvfa_model\n",
    "    # del full_ev_ds\n",
    "\n",
    "\n",
    "    grid = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"svc\", SVC(\n",
    "            C=1,\n",
    "            kernel=\"rbf\",\n",
    "            gamma=\"auto\",\n",
    "            class_weight=\"balanced\",\n",
    "            probability=False,  # set True if you need predict_proba\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    print(\"[LOG] - Classification.\")\n",
    "\n",
    "    # scalerv = StandardScaler()\n",
    "    # X_train_100 = scalerv.fit_transform(X_train_100)\n",
    "    # X_test_100 = scalerv.transform(X_test_100)\n",
    "\n",
    "    grid.fit(X_train_100, Y_train_100)\n",
    "\n",
    "    print(\"----100------\")\n",
    "    print(f\"Train accuracy: {accuracy_score(Y_train_100, grid.predict(X_train_100)) * 100:.2f}%\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(Y_test_100, grid.predict(X_test_100)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----50------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_50)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----10------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_10)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"[LOG]- NUM_OF_GRAPH_EVENTS:\", NUM_OF_GRAPH_EVENTS, \" | DATASET:\", DATASET,\n",
    "          \" | NORMALIZE_FEAT:\", NORMALIZE_FEAT,\n",
    "          \" | R:\", R, \" | D_MAX: \", D_MAX, \" | NOICE_REMOVED: \", NOICE_REMOVED,\n",
    "          \" | NR_BIN_XY_SIZE: \", NR_BIN_XY_SIZE, \" | NR_TIME_BIN_SIZE: \", NR_TIME_BIN_SIZE, \" | NR_MINIMUM_EVENTS: \",\n",
    "          NR_MINIMUM_EVENTS, \" | HV_DIMENTION: \", HV_DIMENTION,\" | LAYERS: \", LAYERS,\" | DELTA: \", DELTA,\" | EQUATION: \", EQUATION,)\n",
    "\n",
    "        # del clf\n"
   ],
   "id": "da2b6131798261ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - parameter initialization.\n",
      "[LOG] - Loading events\n",
      "[LOG] - Making class objects.\n",
      "[LOG] - Loading graph and converting to HVs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graphcnnVSA_Binding_FULL.py:44: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\b\\abs_fakvb73nko\\croot\\pytorch-select_1730848725921\\work\\torch\\csrc\\utils\\tensor_new.cpp:623.)\n",
      "  Adj_block = torch.sparse.FloatTensor(edge_index, Adj_block_elem, torch.Size([num_nodes, num_nodes]))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 99\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(ds_train)):\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;66;03m# print(i)\u001B[39;00m\n\u001B[0;32m     98\u001B[0m     g \u001B[38;5;241m=\u001B[39m MNISTGraph_model_train_100\u001B[38;5;241m.\u001B[39mget(i)\n\u001B[1;32m---> 99\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m \u001B[43mhvs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_hvs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m     X_train_100\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[0;32m    101\u001B[0m     Y_train_100\u001B[38;5;241m.\u001B[39mappend(y)\n",
      "File \u001B[1;32m~\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graph_to_vec_converter.py:25\u001B[0m, in \u001B[0;36mHVs.make_hvs\u001B[1;34m(self, graph)\u001B[0m\n\u001B[0;32m     21\u001B[0m     p_hv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcb\u001B[38;5;241m.\u001B[39mHV_P[\u001B[38;5;28mint\u001B[39m(p_)]\n\u001B[0;32m     23\u001B[0m     all_nodes\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcb\u001B[38;5;241m.\u001B[39mbundle([x_hv, y_hv, t_hv, p_hv]))\n\u001B[1;32m---> 25\u001B[0m graph_hv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgvfa\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_nodes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graph_hv, graph\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\.conda\\envs\\efvoxel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\efvoxel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graphcnnVSA_Binding_FULL.py:215\u001B[0m, in \u001B[0;36mGraphCNN.forward\u001B[1;34m(self, x, edge_index)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;66;03m# Iterating over each layer to update the node features\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 215\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext_layer_eps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mAdj_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAdj_block\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mequation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mequation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     hidden_rep\u001B[38;5;241m.\u001B[39mappend(h)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;66;03m# Stack the hidden representations from all layers along a new dimension\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graphcnnVSA_Binding_FULL.py:170\u001B[0m, in \u001B[0;36mGraphCNN.next_layer_eps\u001B[1;34m(self, h, layer, padded_neighbor_list, Adj_block, delta, equation)\u001B[0m\n\u001B[0;32m    168\u001B[0m pooled \u001B[38;5;241m=\u001B[39m pooled_no_perm\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(delta \u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 170\u001B[0m     pooled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpooled\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m+\u001B[39mh \u001B[38;5;66;03m#self.bind(h,pooled) +  h   #pooled + h  #self.bind(h,pooled) + #\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m(delta \u001B[38;5;241m==\u001B[39m\u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m    172\u001B[0m     pooled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbind(h,pooled)\u001B[38;5;241m+\u001B[39mh\u001B[38;5;241m+\u001B[39mpooled \u001B[38;5;66;03m#self.bind(h,pooled) +  h   #pooled + h  #self.bind(h,pooled) + #\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graphcnnVSA_Binding_FULL.py:110\u001B[0m, in \u001B[0;36mGraphCNN.bind\u001B[1;34m(self, x, y)\u001B[0m\n\u001B[0;32m    107\u001B[0m product \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmul(fft_self, fft_other)\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# Perform inverse FFT to get back to the spatial domain\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mifft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# Return the real part of the result as the final bound hypervectors\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mreal(result)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic classifier - grid search",
   "id": "f3c401d2da34ba90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T13:16:00.714916300Z",
     "start_time": "2025-07-29T12:44:32.588141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.graph_to_vec_converter import  HVs\n",
    "from sklearn.metrics       import accuracy_score, classification_report\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from src.graph_generation import NMNISTGraphDataset\n",
    "from src.loader import ev_loader\n",
    "from src.graphcnnVSA_Binding_FULL import GraphCNN\n",
    "from src.codebook import CodeBook\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "print(\"[LOG] - parameter initialization.\")\n",
    "# GRAPH parameters\n",
    "DATA_NAME = \"SNKTH\" # NCARS, NMNIST\n",
    "DATA_PATH = \"data\"\n",
    "DATASET = \"full\"  # full / test      size of dataset loading for training and testing\n",
    "NORMALIZE_FEAT = False\n",
    "NUM_OF_GRAPH_EVENTS = 100  # None, 10, 50, 100. etc\n",
    "\n",
    "if DATA_NAME == \"SNKTH\":\n",
    "    X_MAX = 360\n",
    "    Y_MAX = 360\n",
    "    T_MAX = 100_000_000\n",
    "    T_STEP = 10_000\n",
    "\n",
    "    R = 7\n",
    "    D_MAX = 30\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = True\n",
    "    NR_BIN_XY_SIZE = 4\n",
    "    NR_TIME_BIN_SIZE = 20_000\n",
    "    NR_MINIMUM_EVENTS = 3\n",
    "\n",
    "if DATA_NAME == \"NCARS\":\n",
    "    X_MAX = 200\n",
    "    Y_MAX = 200\n",
    "    T_MAX = 100_000_000\n",
    "    T_STEP = 10_000\n",
    "\n",
    "    R = 7\n",
    "    D_MAX = 30\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = True\n",
    "    NR_BIN_XY_SIZE = 4\n",
    "    NR_TIME_BIN_SIZE = 20_000\n",
    "    NR_MINIMUM_EVENTS = 3\n",
    "\n",
    "\n",
    "# GVFA parameters\n",
    "HV_DIMENTION = 1000\n",
    "LAYERS = 5\n",
    "DELTA = 1  # 2\n",
    "EQUATION = 11\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# load event streams\n",
    "print(\"[LOG] - Loading events\")\n",
    "# full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\n",
    "ds = ev_loader(root=DATA_PATH, dataset=DATASET, data_name=DATA_NAME)\n",
    "ds_train, ds_test = train_test_split(ds, test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"[LOG] - Making class objects.\")\n",
    "MNISTGraph_model_train_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_train, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MNISTGraph_model_test_50 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=50,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_10 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=10,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "items = [500, 1000, 5000, 7000]\n",
    "for item in items:\n",
    "    HV_DIMENTION = item\n",
    "    gvfa_model = GraphCNN(input_dim=HV_DIMENTION, num_layers=LAYERS, delta=DELTA, graph_pooling_type=\"sum\",\n",
    "                          neighbor_pooling_type=\"sum\", device=DEVICE, equation=EQUATION).to(DEVICE)\n",
    "    cb = CodeBook(dim=HV_DIMENTION, x_max=X_MAX, y_max=Y_MAX, t_max=T_MAX, t_step=T_STEP)\n",
    "    hvs = HVs(codebook=cb, gvfa_model=gvfa_model)\n",
    "\n",
    "\n",
    "    X_train_100, X_test_100, X_test_50,X_test_10, Y_train_100,Y_test_100, y_test_50_10 = [],[],[],[], [],[],[]\n",
    "    print(\"[LOG] - Loading graph and converting to HVs.\")\n",
    "    for i in range(len(ds_train)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_train_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_train_100.append(x)\n",
    "        Y_train_100.append(y)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_test_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_test_100.append(x)\n",
    "        Y_test_100.append(y)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = scaler.fit_transform(X_train_)\n",
    "    # X_test = scaler.fit_transform(X_test_)\n",
    "\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g_50 = MNISTGraph_model_test_50.get(i)\n",
    "        g_10 = MNISTGraph_model_test_10.get(i)\n",
    "\n",
    "        # print(g)\n",
    "\n",
    "        x_50, y = hvs.make_hvs(graph=g_50)\n",
    "        x_10, _ = hvs.make_hvs(graph=g_10)\n",
    "\n",
    "\n",
    "        X_test_50.append(x_50)\n",
    "        X_test_10.append(x_10)\n",
    "        y_test_50_10.append(y)\n",
    "\n",
    "\n",
    "    # X_test_50 = scaler.fit_transform(X_test_50_)\n",
    "    # X_test_10 = scaler.fit_transform(X_test_10_)\n",
    "\n",
    "\n",
    "    del cb\n",
    "    del hvs\n",
    "    del gvfa_model\n",
    "    # del full_ev_ds\n",
    "\n",
    "    el = [1000]\n",
    "\n",
    "    print(\"[LOG] - Classification.\")\n",
    "\n",
    "    # clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.9,degree=6)\n",
    "    pipe_lr = Pipeline([\n",
    "        # scale each feature (especially important for high-dim hypervectors)\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        # multinomial logistic regression via 'saga' or 'lbfgs'\n",
    "        (\"lr\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            max_iter=5000,\n",
    "            tol=1e-4\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"lr__C\":           [0.01, 0.1, 1, 10],\n",
    "        \"lr__penalty\":     [\"l2\"],                  # saga also supports 'l1' or 'elasticnet' if you add l1_ratio\n",
    "        \"lr__solver\":      [\"lbfgs\", \"saga\"],\n",
    "        \"lr__class_weight\":[None, \"balanced\"]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe_lr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                   # 5-fold stratified by default for classification\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train_100, Y_train_100)\n",
    "\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    print(grid.param_grid)\n",
    "\n",
    "    print(\"----100------\")\n",
    "    print(f\"Train accuracy: {accuracy_score(Y_train_100, grid.predict(X_train_100)) * 100:.2f}%\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(Y_test_100, grid.predict(X_test_100)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----50------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_50)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----10------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_10)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"[LOG]- NUM_OF_GRAPH_EVENTS:\", NUM_OF_GRAPH_EVENTS, \" | DATASET:\", DATASET,\n",
    "          \" | NORMALIZE_FEAT:\", NORMALIZE_FEAT,\n",
    "          \" | R:\", R, \" | D_MAX: \", D_MAX, \" | NOICE_REMOVED: \", NOICE_REMOVED,\n",
    "          \" | NR_BIN_XY_SIZE: \", NR_BIN_XY_SIZE, \" | NR_TIME_BIN_SIZE: \", NR_TIME_BIN_SIZE, \" | NR_MINIMUM_EVENTS: \",\n",
    "          NR_MINIMUM_EVENTS, \" | HV_DIMENTION: \", HV_DIMENTION,\" | LAYERS: \", LAYERS,\" | DELTA: \", DELTA,\" | EQUATION: \", EQUATION,)\n",
    "\n",
    "        # del clf\n"
   ],
   "id": "9381cd7803047cc7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - parameter initialization.\n",
      "[LOG] - Loading events\n",
      "[LOG] - Making class objects.\n",
      "[LOG] - Loading graph and converting to HVs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\OneDrive - LA TROBE UNIVERSITY\\Projects\\EventCamera\\classification\\src\\graphcnnVSA_Binding_FULL.py:44: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\b\\abs_fakvb73nko\\croot\\pytorch-select_1730848725921\\work\\torch\\csrc\\utils\\tensor_new.cpp:623.)\n",
      "  Adj_block = torch.sparse.FloatTensor(edge_index, Adj_block_elem, torch.Size([num_nodes, num_nodes]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 0.01, 'lr__class_weight': None, 'lr__penalty': 'l2', 'lr__solver': 'saga'}\n",
      "0.5765933283612449\n",
      "{'lr__C': [0.01, 0.1, 1, 10], 'lr__penalty': ['l2'], 'lr__solver': ['lbfgs', 'saga'], 'lr__class_weight': [None, 'balanced']}\n",
      "----100------\n",
      "Train accuracy: 68.14%\n",
      "Test  accuracy: 60.70%\n",
      "----50------\n",
      "Test  accuracy: 56.28%\n",
      "----10------\n",
      "Test  accuracy: 55.58%\n",
      "[LOG]- NUM_OF_GRAPH_EVENTS: 100  | DATASET: full  | NORMALIZE_FEAT: False  | R: 7  | D_MAX:  30  | NOICE_REMOVED:  True  | NR_BIN_XY_SIZE:  4  | NR_TIME_BIN_SIZE:  20000  | NR_MINIMUM_EVENTS:  3  | HV_DIMENTION:  500  | LAYERS:  5  | DELTA:  1  | EQUATION:  11\n",
      "[LOG] - Loading graph and converting to HVs.\n",
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 0.01, 'lr__class_weight': None, 'lr__penalty': 'l2', 'lr__solver': 'saga'}\n",
      "0.5655180012204217\n",
      "{'lr__C': [0.01, 0.1, 1, 10], 'lr__penalty': ['l2'], 'lr__solver': ['lbfgs', 'saga'], 'lr__class_weight': [None, 'balanced']}\n",
      "----100------\n",
      "Train accuracy: 75.54%\n",
      "Test  accuracy: 56.98%\n",
      "----50------\n",
      "Test  accuracy: 56.98%\n",
      "----10------\n",
      "Test  accuracy: 56.74%\n",
      "[LOG]- NUM_OF_GRAPH_EVENTS: 100  | DATASET: full  | NORMALIZE_FEAT: False  | R: 7  | D_MAX:  30  | NOICE_REMOVED:  True  | NR_BIN_XY_SIZE:  4  | NR_TIME_BIN_SIZE:  20000  | NR_MINIMUM_EVENTS:  3  | HV_DIMENTION:  1000  | LAYERS:  5  | DELTA:  1  | EQUATION:  11\n",
      "[LOG] - Loading graph and converting to HVs.\n",
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22390013@students.ltu.edu.au\\.conda\\envs\\efvoxel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 0.01, 'lr__class_weight': 'balanced', 'lr__penalty': 'l2', 'lr__solver': 'lbfgs'}\n",
      "0.5690250186453318\n",
      "{'lr__C': [0.01, 0.1, 1, 10], 'lr__penalty': ['l2'], 'lr__solver': ['lbfgs', 'saga'], 'lr__class_weight': [None, 'balanced']}\n",
      "----100------\n",
      "Train accuracy: 95.17%\n",
      "Test  accuracy: 56.51%\n",
      "----50------\n",
      "Test  accuracy: 59.30%\n",
      "----10------\n",
      "Test  accuracy: 55.35%\n",
      "[LOG]- NUM_OF_GRAPH_EVENTS: 100  | DATASET: full  | NORMALIZE_FEAT: False  | R: 7  | D_MAX:  30  | NOICE_REMOVED:  True  | NR_BIN_XY_SIZE:  4  | NR_TIME_BIN_SIZE:  20000  | NR_MINIMUM_EVENTS:  3  | HV_DIMENTION:  5000  | LAYERS:  5  | DELTA:  1  | EQUATION:  11\n",
      "[LOG] - Loading graph and converting to HVs.\n",
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
