{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-31T02:35:22.464931500Z",
     "start_time": "2025-07-31T02:15:33.238201Z"
    }
   },
   "source": [
    "from src.graph_to_vec_converter import  HVs\n",
    "from sklearn.metrics       import accuracy_score, classification_report\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from src.graph_generation import NMNISTGraphDataset\n",
    "from src.loader import ev_loader\n",
    "from src.graphcnnVSA_Binding_FULL import GraphCNN\n",
    "from src.codebook import CodeBook\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.metrics         import accuracy_score\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "print(\"[LOG] - parameter initialization.\")\n",
    "# GRAPH parameters\n",
    "DATA_NAME = \"SNKTH\" # NCARS, NMNIST\n",
    "DATA_PATH = \"data\"\n",
    "DATASET = \"full\"  # full / test      size of dataset loading for training and testing\n",
    "NORMALIZE_FEAT = False\n",
    "NUM_OF_GRAPH_EVENTS = 100  # None, 10, 50, 100. etc\n",
    "\n",
    "\n",
    "if DATA_NAME == \"SNKTH\":\n",
    "    X_MAX = 360\n",
    "    Y_MAX = 360\n",
    "    T_MAX = 1_000_000\n",
    "    T_STEP = 10_0\n",
    "\n",
    "    R = 2\n",
    "    D_MAX = 4\n",
    "\n",
    "    # NOISE parameters\n",
    "    NOICE_REMOVED = True\n",
    "    NR_BIN_XY_SIZE = 5\n",
    "    NR_TIME_BIN_SIZE = 20_00\n",
    "    NR_MINIMUM_EVENTS = 2\n",
    "\n",
    "\n",
    "\n",
    "# GVFA parameters\n",
    "# HV_DIMENTION = 5000\n",
    "LAYERS = 5\n",
    "DELTA = 1  # 2\n",
    "EQUATION = 11\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# load event streams\n",
    "print(\"[LOG] - Loading events\")\n",
    "# full_ev_ds = ev_loader(root=DATA_PATH, dataset=DATASET)\n",
    "ds = ev_loader(root=DATA_PATH, dataset=DATASET, data_name=DATA_NAME)\n",
    "ds_train, ds_test = train_test_split(ds, test_size=0.2, random_state=42,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"[LOG] - Making class objects.\")\n",
    "MNISTGraph_model_train_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_train, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_100 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=NUM_OF_GRAPH_EVENTS,\n",
    "                                      R=R, Dmax=D_MAX,\n",
    "                                      noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                      nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                      nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MNISTGraph_model_test_50 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=50,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "MNISTGraph_model_test_10 = NMNISTGraphDataset(tonic_raw_dataset=ds_test, num_of_graph_events=10,\n",
    "                                            R=R, Dmax=D_MAX,\n",
    "                                            noise_remove=NOICE_REMOVED, normalized_feat=NORMALIZE_FEAT,\n",
    "                                            nr_bin_xy_size=NR_BIN_XY_SIZE, nr_minimum_events=NR_MINIMUM_EVENTS,\n",
    "                                            nr_time_bin_size=NR_TIME_BIN_SIZE)\n",
    "\n",
    "HV_Dimensions = [5000, 7000]\n",
    "for item in HV_Dimensions:\n",
    "    HV_DIMENTION = item\n",
    "    gvfa_model = GraphCNN(input_dim=HV_DIMENTION, num_layers=LAYERS, delta=DELTA, graph_pooling_type=\"sum\",\n",
    "                          neighbor_pooling_type=\"sum\", device=DEVICE, equation=EQUATION).to(DEVICE)\n",
    "    cb = CodeBook(dim=HV_DIMENTION, x_max=X_MAX, y_max=Y_MAX, t_max=T_MAX, t_step=T_STEP)\n",
    "    hvs = HVs(codebook=cb, gvfa_model=gvfa_model)\n",
    "\n",
    "\n",
    "    X_train_100, X_test_100, X_test_50,X_test_10, Y_train_100,Y_test_100, y_test_50_10 = [],[],[],[], [],[],[]\n",
    "    print(\"[LOG] - Loading graph and converting to HVs.\")\n",
    "    for i in range(len(ds_train)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_train_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_train_100.append(x)\n",
    "        Y_train_100.append(y)\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g = MNISTGraph_model_test_100.get(i)\n",
    "        x, y = hvs.make_hvs(graph=g)\n",
    "        X_test_100.append(x)\n",
    "        Y_test_100.append(y)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = scaler.fit_transform(X_train_)\n",
    "    # X_test = scaler.fit_transform(X_test_)\n",
    "\n",
    "    for i in range(len(ds_test)):\n",
    "        # print(i)\n",
    "        g_50 = MNISTGraph_model_test_50.get(i)\n",
    "        g_10 = MNISTGraph_model_test_10.get(i)\n",
    "\n",
    "        # print(g)\n",
    "\n",
    "        x_50, y = hvs.make_hvs(graph=g_50)\n",
    "        x_10, _ = hvs.make_hvs(graph=g_10)\n",
    "\n",
    "\n",
    "        X_test_50.append(x_50)\n",
    "        X_test_10.append(x_10)\n",
    "        y_test_50_10.append(y)\n",
    "\n",
    "\n",
    "    # X_test_50 = scaler.fit_transform(X_test_50_)\n",
    "    # X_test_10 = scaler.fit_transform(X_test_10_)\n",
    "\n",
    "\n",
    "    del cb\n",
    "    del hvs\n",
    "    del gvfa_model\n",
    "    # del full_ev_ds\n",
    "\n",
    "\n",
    "    print(\"[LOG] - Classification.\")\n",
    "\n",
    "    # clf = SVC(kernel=\"rbf\", C=0.1, gamma=0.9,degree=6)\n",
    "    pipe_lr = Pipeline([\n",
    "        # scale each feature (especially important for high-dim hypervectors)\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        # multinomial logistic regression via 'saga' or 'lbfgs'\n",
    "        (\"lr\", LogisticRegression(\n",
    "            multi_class=\"multinomial\",\n",
    "            max_iter=5000,\n",
    "            tol=1e-4\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"lr__C\":           [0.01, 0.1, 1, 10],\n",
    "        \"lr__penalty\":     [\"l2\"],                  # saga also supports 'l1' or 'elasticnet' if you add l1_ratio\n",
    "        \"lr__solver\":      [\"lbfgs\", \"saga\"],\n",
    "        \"lr__class_weight\":[None, \"balanced\"]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe_lr,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                   # 5-fold stratified by default for classification\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(X_train_100, Y_train_100)\n",
    "\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_score_)\n",
    "    print(grid.param_grid)\n",
    "\n",
    "    print(\"----100------\")\n",
    "    print(f\"Train accuracy: {accuracy_score(Y_train_100, grid.predict(X_train_100)) * 100:.2f}%\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(Y_test_100, grid.predict(X_test_100)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----50------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_50)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"----10------\")\n",
    "    print(f\"Test  accuracy: {accuracy_score(y_test_50_10, grid.predict(X_test_10)) * 100:.2f}%\")\n",
    "\n",
    "    print(\"[LOG]- NUM_OF_GRAPH_EVENTS:\", NUM_OF_GRAPH_EVENTS, \" | DATASET:\", DATASET,\n",
    "          \" | NORMALIZE_FEAT:\", NORMALIZE_FEAT,\n",
    "          \" | R:\", R, \" | D_MAX: \", D_MAX, \" | NOICE_REMOVED: \", NOICE_REMOVED,\n",
    "          \" | NR_BIN_XY_SIZE: \", NR_BIN_XY_SIZE, \" | NR_TIME_BIN_SIZE: \", NR_TIME_BIN_SIZE, \" | NR_MINIMUM_EVENTS: \",\n",
    "          NR_MINIMUM_EVENTS, \" | HV_DIMENTION: \", HV_DIMENTION,\" | LAYERS: \", LAYERS,\" | DELTA: \", DELTA,\" | EQUATION: \", EQUATION,)\n",
    "\n",
    "        # del clf\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] - parameter initialization.\n",
      "[LOG] - Loading events\n",
      "[LOG] - Making class objects.\n",
      "[LOG] - Loading graph and converting to HVs.\n",
      "[LOG] - Classification.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
